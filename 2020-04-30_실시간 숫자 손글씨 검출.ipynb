{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 서론\n",
    "---\n",
    "\n",
    "## 주제: 실시간 숫자 손글씨 검출\n",
    "참여 연구자: 김동주, 이도경\n",
    "\n",
    "## 요약:\n",
    "OpenCV를 이용하여 동영상으로 부터 손글씨를 검출한다.  \n",
    "모델 학습에는 두 가지 방법을 사용한다.  \n",
    "\n",
    "1. MNIST 데이터베이스를 이용하여 모델을 지도\n",
    "2. 레이블 되어진 동영상을 이용하여 모델을 지도 (직접 획득한 데이터를 이용)\n",
    "  \n",
    "두 모델에 대한 탐구를 통하여 다음의 목표를 달성하고자 한다.\n",
    "\n",
    "* 영상처리를 이용한 실시간 분류기에 대한 접근 및 이해  \n",
    "* 향후 연구방향 제시"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 본론\n",
    "---\n",
    "\n",
    "## 목표: 동영상을 이용한 Machine Learning 지도학습 데이터 수집\n",
    "\n",
    "## 설계:\n",
    "본 프로그램의 구현 과정은 크게 3가지 단계로 이루어집니다.\n",
    "\n",
    "1. raw 데이터 획득\n",
    "    * 균일한 색상의 배경과 특정색상 선을 사용하여 배경과 검출하고자 하는 숫자를 명확하게 함. (화이트보드/붉은색 마커 사용)\n",
    "    * 녹화된 영상의 파일명에 해당 숫자 값을 명시하여 레이블함. (ex: `<정답>_<영상번호>.mov`)\n",
    "\n",
    "2. 데이터셋 준비\n",
    "    * OpenCV를 이용하여 데이터를 28x28, Single-channel Image로 통일시킴\n",
    "    * 데이터에 올바른 레이블을 매칭시켜줌.\n",
    "\n",
    "3. 머신러닝 모델에 적용\n",
    "    * TensorFlow 시스템을 이용하여 모델 구현, 학습, 그리고 검증"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "    \n",
    "## 1. raw 데이터 획득\n",
    "\n",
    "가장 먼저 해야할 일은 역시 데이터 수집입니다.  \n",
    "힘들지만 손수 글씨를 써가며 촬영하는 방식으로 데이터를 획득하였습니다.\n",
    "\n",
    "촬영한 영상 원본은 다음 링크에서 확인하실 수 있습니다.\n",
    "\n",
    "* Google Drive URL: https://drive.google.com/drive/folders/1-HyJBGEiGAk_lxCEdQ6mXYBZVoAfACZt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. 데이터셋 준비\n",
    "\n",
    "### 2.1 데이터 정규화\n",
    "\n",
    "다음의 코드를 이용하여 정규화(: 화면 자르기, 손글씨 검출, 노이즈제거, 차원축소, 크기축소) 작업을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# this code normalizes raw video files into 28x28 single-channel images\n",
    "# \n",
    "# (C) 2020 Kim Dong Joo, Dongguk University, Gyeongju\n",
    "# email hepheir@gmail.com\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "RAW_DATA_PATH = 'res/raw/'\n",
    "OUT_DATA_PATH = 'res/out/'\n",
    "\n",
    "def main():\n",
    "    ls = os.listdir(RAW_DATA_PATH)\n",
    "\n",
    "    usr_sel = input(\"%d files found.\\nproceed? [y/n]: \" % len(ls))\n",
    "    if usr_sel != 'y':\n",
    "        print(\"Canceled by user.\")\n",
    "        return\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "\n",
    "    for file_index in range(len(ls)):\n",
    "        filename = ls[file_index]\n",
    "\n",
    "        if filename.startswith('TEST'): continue\n",
    "\n",
    "        video = cv2.VideoCapture(RAW_DATA_PATH + filename)\n",
    "        frame_index = 0\n",
    "        \n",
    "        while video.isOpened():\n",
    "            ret, frame = video.read()\n",
    "            if not ret: break\n",
    "\n",
    "            # -----------------------------------------------------------\n",
    "            # Cut & Resize\n",
    "            #   * why [:,420:-420]?\n",
    "            #   --> frame.shape of the used videos are 1920x1080.\n",
    "            #       to make it square, sliced 'x' from each frames from 420 to -420\n",
    "            #       (1080, 1920, 3) -> (1080, 1080, 3)\n",
    "            #   * why resize to (488,488)?\n",
    "            #   --> to fasten the process.\n",
    "            #       chose 448, since it is 28 multiplied by 2^4\n",
    "            #       (448=28*(2^4))\n",
    "            # -----------------------------------------------------------\n",
    "            frame = frame[::-1,-420:420:-1] \n",
    "            frame = cv2.resize(frame, (448,448))\n",
    "\n",
    "            # -----------------------------------------------------------\n",
    "            # Detect hand writings\n",
    "            # -----------------------------------------------------------\n",
    "            yuv_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2YUV)\n",
    "            red_yuv_range = (( 64,  0,128),\n",
    "                             (255,128,255))\n",
    "            red_yuv_mask = cv2.inRange(yuv_frame, red_yuv_range[0], red_yuv_range[1])\n",
    "\n",
    "            hsv_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "            red_hsv_range = ((  0, 64, 64),\n",
    "                             (255,255,255))\n",
    "            red_hsv_mask = cv2.inRange(hsv_frame, red_hsv_range[0], red_hsv_range[1])\n",
    "            \n",
    "            red_mask = cv2.bitwise_and(red_yuv_mask, red_hsv_mask)\n",
    "\n",
    "            # -----------------------------------------------------------\n",
    "            # Dilate mask to make them survive from resizing\n",
    "            # -----------------------------------------------------------\n",
    "            red_mask = cv2.dilate(red_mask,\n",
    "                                  cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (9,9)),\n",
    "                                  iterations=3)\n",
    "\n",
    "            # -----------------------------------------------------------\n",
    "            # Resize and export frames\n",
    "            # -----------------------------------------------------------\n",
    "            out_frame = cv2.resize(red_mask, (28, 28))\n",
    "            out_filename = OUT_DATA_PATH + filename.replace('.MOV', '_%d.png' % frame_index)\n",
    "\n",
    "            cv2.imwrite(out_filename, out_frame)\n",
    "            frame_index += 1\n",
    "\n",
    "            print(\"\\rCreating (%d/%d)... %s         \" % (file_index+1, len(ls), out_filename), end=\"\")\n",
    "        video.release()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 레이블 매칭\n",
    "\n",
    "다음은 벤치마킹할 mnist의 손글씨 검출 학습데이터입니다.\n",
    "이 예제를 통해 데이터셋이 어떠한 형태로 저장되는지 알아볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "print('x_train[0]\\n', x_train[0])\n",
    "print('y_train[0]\\n', y_train[0])\n",
    "\n",
    "cv2.imshow(y_train[0], x_train[0])\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞선 예제를 통해 데이터셋이 어떠한 형태로 주어지는지를 보았습니다.\n",
    "`x_train`, `y_train`에는 각각 (28x28xN), (label x N)의 형태로 데이터와 레이블이 존재하며 1:1 대응을 이루게 됩니다.\n",
    "  \n",
    "이를 바탕으로 '2. 데이터 정규화'에서 획득한 이미지들의 레이블링을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "DATA_PATH = 'res/out/'\n",
    "\n",
    "x_train, y_train = [], []\n",
    "\n",
    "for filename in os.listdir(DATA_PATH):\n",
    "    if filename.startswith('TEST'): continue # 레이블링이 불가능한 영상은 제외\n",
    "\n",
    "    frame = cv2.imread(DATA_PATH + filename)\n",
    "    label = filename.split('_')[0]\n",
    "    \n",
    "    x_train.append(frame)\n",
    "    y_train.append(label)\n",
    "\n",
    "x_train = np.array(x_train, dtype=np.uint8)\n",
    "y_train = np.array(y_train, dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##  3. 실제 모델에 적용\n",
    "\n",
    "이번에는 mnist 데이터셋을 이용하여 학습시킨 모델에 사용자 데이터셋('3. 레이블 매칭'에서 생성)을 적용시켜 볼 것입니다.  \n",
    "\n",
    "### 3.1 mnist 데이터셋으로 학습시킨 모델\n",
    "우선은 mnist 데이터셋을 이용하여 모델을 학습시키고 결과를 적용합니다. [모델 저장과 복원 참고](https://www.tensorflow.org/tutorials/keras/save_and_load?hl=ko)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 60000 samples\nEpoch 1/5\n60000/60000 [==============================] - 5s 86us/sample - loss: 0.2949 - accuracy: 0.9141\nEpoch 2/5\n60000/60000 [==============================] - 5s 79us/sample - loss: 0.1415 - accuracy: 0.9578\nEpoch 3/5\n60000/60000 [==============================] - 6s 96us/sample - loss: 0.1078 - accuracy: 0.9671\nEpoch 4/5\n60000/60000 [==============================] - 6s 105us/sample - loss: 0.0898 - accuracy: 0.9717\nEpoch 5/5\n60000/60000 [==============================] - 5s 87us/sample - loss: 0.0748 - accuracy: 0.9764\n10000/10000 - 0s - loss: 0.0732 - accuracy: 0.9778\n"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "checkpoint = 'models/checkpoints/mnist_tmp'\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Dense(10, activation='softmax')\n",
    "        ])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "model.evaluate(x_test, y_test, verbose=2)\n",
    "\n",
    "model.save_weights(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 저장한 모델을 불러오기\n",
    "다음 코드를 통해 모델이 제대로 저장되었는지 확인해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "3281/3281 - 0s - loss: 12.7142 - accuracy: 0.1439\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[12.71417108783995, 0.14385858]"
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from models import mnist\n",
    "\n",
    "model = mnist.model # 3.1 에서 사용한 모델\n",
    "\n",
    "model.load_weights(mnist.checkpoint)\n",
    "model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 사용자 데이터셋으로 테스트\n",
    "모델이 제대로 저장되었음을 확인했으니, 모델에 준비해온 데이터셋을 적용해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "3281/3281 [==============================] - 0s 105us/sample - loss: 17.1309 - accuracy: 0.1344\n"
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from models.mnist import model, checkpoint\n",
    "\n",
    "DATA_PATH = './res/out/'\n",
    "\n",
    "def load_user_data():\n",
    "    x, y = [], []\n",
    "    for filename in os.listdir(DATA_PATH):\n",
    "        if filename.startswith('TEST'): continue\n",
    "        \n",
    "        frame = cv2.imread(DATA_PATH + filename, cv2.IMREAD_GRAYSCALE)\n",
    "        label = filename.split('_')[0]\n",
    "        \n",
    "        if type(frame) is type(None): continue # To sort out dummy files as '.DS_Store'\n",
    "        if not frame.any(): continue # Empty frame\n",
    "\n",
    "        x.append(frame.copy())\n",
    "        y.append(label)\n",
    "\n",
    "    x = np.array(x, dtype=np.uint8)\n",
    "    y = np.array(y, dtype=np.uint8)\n",
    "    return x/255.0, y\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    x_test, y_test = load_user_data()\n",
    "\n",
    "    model.load_weights(checkpoint)\n",
    "    model.evaluate(x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정말 아쉽게도 14.39%의 정확성밖에 보이지 못하였습니다.  \n",
    "  \n",
    "아마도 사용자 데이터 중에서 글씨가 덜 써진, 혹은 기타 노이즈(ex. 글씨외에도 손이나 팔뚝이 검출)로 인하여 올바르게 레이블이 되어지지 않은 프레임들이 문제가 되지 않나 싶습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 실시간 손글씨 검출\n",
    "\n",
    "보다 직관적인 결과 확인을 위해, 동영상에서 실시간으로 손글씨 분류 결과를 확인 할 수 있는 모델을 생성해보기로 합니다.  \n",
    "위 모델에서 샘플당 처리소요시간은 74us로, 실시간 처리에는 큰 지장이 없으리라 생각이됩니다.\n",
    "\n",
    "### 4.1 카메라 이용 및 녹화\n",
    "\n",
    "우선은 카메라를 이용하는 방법과 영상을 녹화하는 방법에 대하여 간단히 훑어봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# demonstrate how to record a video with a camera using opencv-python\n",
    "# \n",
    "# (C) 2020 Kim Dong Joo, Dongguk University, Gyeongju\n",
    "# email hepheir@gmail.com\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "import cv2\n",
    "from datetime import datetime\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Video I/O Settings\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "FRAME_WIDTH, FRAME_HEIGHT = 256, 256\n",
    "FPS = 20.0\n",
    "\n",
    "# 녹화된 영상이 저장될 곳\n",
    "FILE_NAME_PATTERN = \"res/records/\"+\"base-%y%m%d_%H%M%S.avi\"\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Utilities\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "status = {\n",
    "    'frames': 0,\n",
    "    'video_length': 0}\n",
    "\n",
    "def updateStatus():\n",
    "    # Put all the values to be updated for each cycles\n",
    "    global status\n",
    "    status['frames'] += 1\n",
    "    status['video_length'] = status['frames'] // FPS\n",
    "\n",
    "def showStatus():\n",
    "    global status\n",
    "    msg = \"\\r\"\n",
    "    for key in status:\n",
    "        msg += \"%s: %s \" % (key, str(status[key]))\n",
    "    print(msg, end=\"\")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Main loop\n",
    "# -----------------------------------------------------------\n",
    "if __name__ == '__main__':\n",
    "    vidIn = cv2.VideoCapture(0)\n",
    "    vidIn.set(cv2.CAP_PROP_FRAME_WIDTH,  FRAME_WIDTH)\n",
    "    vidIn.set(cv2.CAP_PROP_FRAME_HEIGHT, FRAME_HEIGHT)\n",
    "\n",
    "    vidOut = cv2.VideoWriter(datetime.now().strftime(FILE_NAME_PATTERN),\n",
    "                             cv2.VideoWriter_fourcc(*'MJPG'),\n",
    "                             FPS,\n",
    "                             (FRAME_WIDTH, FRAME_HEIGHT))\n",
    "\n",
    "    while vidIn.isOpened():\n",
    "        ret, frame = vidIn.read()\n",
    "        if not ret: break\n",
    "\n",
    "        frame = cv2.resize(frame, (FRAME_HEIGHT, FRAME_WIDTH))\n",
    "        # -----------------------------------------------------------\n",
    "        # TODO\n",
    "        # -----------------------------------------------------------\n",
    "        cv2.imshow('cam', frame)\n",
    "        vidOut.write(frame)\n",
    "        \n",
    "        # -----------------------------------------------------------\n",
    "        # Key mappings\n",
    "        # -----------------------------------------------------------\n",
    "        key = cv2.waitKey(20) & 0xFF\n",
    "        if key == 27: break # ESC\n",
    "\n",
    "        # -----------------------------------------------------------\n",
    "        # Debugs\n",
    "        # -----------------------------------------------------------\n",
    "        updateStatus()\n",
    "        showStatus()\n",
    "\n",
    "    vidIn.release()\n",
    "    vidOut.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "print(\"\")\n",
    "print(\"Successfully Closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 4.2 실시간 예측 모델\n",
    "\n",
    "다음은 카메라 장치를 이용하여 영상을 받아오고, 실시간으로 손글씨를 검출, 정규화하는 스크립트입니다. \n",
    "학습된 모델을 사용하여 정규화한 데이터의 값을 예측합니다.  \n",
    "  \n",
    "만약 모델이 만족스럽지 못한 예측을 할 때를 대비하여, 재훈련 기능도 넣었습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "\nSuccessfully Closed\n"
    }
   ],
   "source": [
    "# -----------------------------------------------------------\n",
    "# demonstrate how to classify hand writings from a video stream\n",
    "# being captured in real-time, using opencv-python\n",
    "# \n",
    "# (C) 2020 Kim Dong Joo, Dongguk University, Gyeongju\n",
    "# email hepheir@gmail.com\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# 모델을 바꾸려면 여기서 수정.\n",
    "from models.mnist import model, checkpoint, input_shape\n",
    "\n",
    "def writeText(frame, txt):\n",
    "    frame[:28,:] = frame[:28,:] // 2\n",
    "    cv2.putText(frame,\n",
    "                txt,\n",
    "                (32, 16),               # Coordinates\n",
    "                cv2.FONT_HERSHEY_PLAIN, # \n",
    "                1.2,                    # Font scale\n",
    "                (128, 128, 255),        # Font color\n",
    "                lineType=cv2.LINE_AA)\n",
    "\n",
    "def getNumericKey(key):\n",
    "    # 0~9사이의 키가 눌렸을 때만 해당 키를 반환.\n",
    "    # 그 외의 키가 눌리면 None을 반환한다.\n",
    "    c = chr(key)\n",
    "    if c in \"0123456789\":\n",
    "        return int(c)\n",
    "    return None\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model.load_weights(checkpoint)\n",
    "\n",
    "    # Re-train settings\n",
    "    if input(\"Load weights?[y/n] :\") == 'y':\n",
    "        model.load_weights(checkpoint+'_tmp')\n",
    "    doSave = input(\"Save weights?[y/n] :\") == 'y'\n",
    "\n",
    "    FRAME_WIDTH, FRAME_HEIGHT = 448, 448\n",
    "    # vidIn = cv2.VideoCapture(0)\n",
    "    vidIn = cv2.VideoCapture('resources/raw/TEST.MOV')\n",
    "\n",
    "    # ===========================================================\n",
    "    # Main loop\n",
    "    # ===========================================================\n",
    "    while vidIn.isOpened():\n",
    "        ret, frame = vidIn.read()\n",
    "        if not ret: break\n",
    "\n",
    "        frame = frame[:,420:-420] # frame[::-1,-420:420:-1]\n",
    "        frame = cv2.resize(frame, (FRAME_HEIGHT, FRAME_WIDTH))\n",
    "        \n",
    "        frame = cv2.flip(frame, 0) # flip virtically (up and down)\n",
    "        frame = cv2.flip(frame, 1) # flip horizontally (left and right)\n",
    "\n",
    "        # -----------------------------------------------------------\n",
    "        # Detect hand writings & normalize\n",
    "        # -----------------------------------------------------------\n",
    "        yuv_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2YUV)\n",
    "        red_yuv_mask = cv2.inRange(yuv_frame,\n",
    "                                   ( 64,  0,128),\n",
    "                                   (255,128,255))\n",
    "\n",
    "        hsv_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "        red_hsv_mask = cv2.inRange(hsv_frame,\n",
    "                                   (  0, 48, 48),\n",
    "                                   (255,255,255))\n",
    "        red_mask = cv2.dilate(cv2.bitwise_and(red_yuv_mask, red_hsv_mask),\n",
    "                              cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (9,9)),\n",
    "                              iterations=3)\n",
    "        normalized_mask = cv2.resize(red_mask,(28,28))\n",
    "\n",
    "        # -----------------------------------------------------------\n",
    "        # Predict using pre-trained model\n",
    "        # -----------------------------------------------------------s\n",
    "        x = np.expand_dims(normalized_mask.reshape(input_shape), axis=0)\n",
    "        p = model.predict(x)[0] # predicts 1 case only.\n",
    "\n",
    "        y = np.where(p == max(p))[0][0] # 가끔 p가 one-hot encoded가 아닌 잡음이 섞여나오는 문제를 해결\n",
    "\n",
    "        # -----------------------------------------------------------\n",
    "        # Key mappings\n",
    "        # -----------------------------------------------------------\n",
    "        key = cv2.waitKey(20) & 0xFF\n",
    "        if key == 27: break # ESC\n",
    "\n",
    "        # -----------------------------------------------------------\n",
    "        # Re-train model (if the prediction is wrong)\n",
    "        # -----------------------------------------------------------\n",
    "        x_train = x\n",
    "        y_train = np.expand_dims(getNumericKey(key), axis=0)\n",
    "\n",
    "        if y_train[0] != None: # 레이블이 주어지면 학습\n",
    "            model.fit(x_train, y_train, epochs=1)\n",
    "\n",
    "        # -----------------------------------------------------------\n",
    "        # Show the result on screen\n",
    "        # -----------------------------------------------------------\n",
    "        writeText(frame, \"Predict: %d, Label : %s\" % (y, str(y_train[0])))\n",
    "        frame[:28,:28] = x[0]\n",
    "        cv2.imshow('Camera', frame)\n",
    "\n",
    "    vidIn.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Re-train on demand\n",
    "    if doSave:\n",
    "        model.save_weights(checkpoint+'_tmp')\n",
    "        print(\"Weights saved.\")\n",
    "\n",
    "print(\"\")\n",
    "print(\"Successfully Closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 결론\n",
    "---\n",
    "\n",
    "## 실험결과\n",
    "\n",
    "손 글씨 검출 모델의 예측결과가 생각보다 좋지 않았습니다.  \n",
    "  \n",
    "글자가 제대로 인식되는 경우모다 엉뚱한 숫자가 예측되는 경우가 압도적으로 많았습니다.\n",
    "  \n",
    "  \n",
    "  \n",
    "## 낮은 정확도의 원인 추측\n",
    "  \n",
    "처음에는 낮은 정확도가 나오는 원인이 다음과 같지 않을까 생각하였습니다.  \n",
    "  \n",
    "* 모델의 사전 훈련이 충분치 않았다\n",
    "* 모델이 너무 단순하다\n",
    "\n",
    "### 모델의 사전 훈련이 충분치 않았다?\n",
    "  \n",
    "우선 모델의 사전 학습 단계에서의 epoch 수를 6 에서 12 이상으로 증가시켜 보았습니다.  \n",
    "그러나 `models/mnist.py`의 성능은 97~98%대에서 머무르는 등 큰 성능의 변화를 기대할 수 없었습니다.    \n",
    "\n",
    "\n",
    "### 모델이 너무 단순하다?\n",
    "\n",
    "mnist 데이터 셋을 이용한 다양한 예제들 중에는 분명 `models/mnist.py`보다 더 복잡해보이는 모델도 많이 있었습니다.  \n",
    "다양한 예제에서 사용되는 `Conv2D()` 층을 사용하여 모델을 설계해보기로 합니다.  \n",
    "우선은 사용하는 모델을 2-layer CNN 모델로(`models/mnist_2_layers.py`)로 수정한 후, 학습을 진행해보기로 하였습니다.  \n",
    "  \n",
    "  \n",
    "`epoch=8`에서 `model.evalutate()`을 이용한 성능평가시에는 98%의 중후반의 정확도를 유지했습니다. 조금 더 fitting 시의 epoch를 늘리면 정확도 99%로의 도달을 조금은 낙관적으로 기대해보았습니다...만, 아쉽게도 모델의 최종 평가 및 체크포인트 생성 과정에서 반복하여 프로그램 에러가 발생하였습니다.  \n",
    "분명히 완벽하게 프로그래밍을 했다고 생각했는데 말이죠.\n",
    "\n",
    "```shell\n",
    "ValueError: Error when checking input: expected conv2d_input to have 4 dimensions, but got array with shape (10000, 28, 28)\n",
    "```\n",
    "  \n",
    "\n",
    "## 실패 원인 분석\n",
    "  \n",
    "다음 두 가지 질문에 초점을 맞추어 문제를 해결해보기로 하였습니다.\n",
    "* '낮은 정확도가 나오는 원인이 무엇인가'\n",
    "* '내가 CNN 모델 층을 어떻게 이해하고 구현하고 있는가'\n",
    "\n",
    "### 실패 원인 분석 중 알게 된 것\n",
    "\n",
    "실패 원인을 분석, 모델을 개선하기 위해 알아보던 중, 다음과 같은 코멘트를 보게 되었습니다\n",
    "> [정이교 @ 페이스북 커뮤니티::케라스코리아](https://www.facebook.com/groups/KerasKorea/3534408366575103/?notif_id=1588753428559249&notif_t=group_nf_highlights)  \n",
    "> reshape로 보통 같은 이미지를 만들고 학습합니다. \n",
    "> 위에서 언급해주셨듯, crop이나 rotation, filp flop등을 사용해서 데이터 개수를 늘려서 정확도를 늘리는 학습을 시킵니다.\n",
    "> data augmentation을 찾아보시면 될거에요. 추가로 gaussian noise를 넣기도 합니다.\n",
    "> 다른 크기의 이미지를 학습시킬때에는, input을 두가지로 정의해주고 네트워크 안에서 dimension을 dense 등으로 맞춰주어야합니다.\n",
    "\n",
    "또한 지난 세미나 발표들 중 오재호 연구원께서 소개해주셨던 선형변환에 강한 SIFT, SURF 알고리즘에 대한 것도 머릿속을 스쳐 지나갑니다. \n",
    "\n",
    "  \n",
    "\n",
    "## * 향후 연구 계획\n",
    "\n",
    "그리하여, 다음주에는 동일한 주제와 목표를 유지하면서, 성능향상을 위한 방법에 대한 학습을 해보려 합니다.  \n",
    "조금은 부실했던, 머신러닝; 특히 CNN과 다양한 종류의 층, 모델을 어떻게 설계해야하는지 대한 부족했던 이해를 메꾸어 보려고 합니다.  \n",
    "  \n",
    "\n",
    "당장 생각하고 있는건, 손글씨 영상의 학습데이터 생성시, 선형변환을 고려한 데이터셋 생성 및 더 견고한(다층) 모델을 안정적으로 시험해 보는 것입니다.  \n",
    "(좌우반전, 회전, 스케일링을 고의적으로 적용하여 하나의 이미지로부터 다양한 데이터셋을 만듬)\n",
    "\n",
    "---\n",
    "\n",
    "# 부록\n",
    "---\n",
    "## 참고\n",
    "* Teachable Machine: 심태섭 연구원이 소개해준 온라인 머신러닝 서비스로, 본 활동에 영감을 받음\n",
    "* "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('ml': conda)",
   "language": "python",
   "name": "python37764bitmlconda56395923f6734d2e9d871b41acf277cc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}