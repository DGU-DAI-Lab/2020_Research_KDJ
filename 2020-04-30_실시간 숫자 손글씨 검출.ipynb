{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 서론\n",
    "---\n",
    "\n",
    "## 주제: 실시간 숫자 손글씨 검출\n",
    "참여 연구자: 김동주, 이도경\n",
    "\n",
    "## 요약:\n",
    "OpenCV를 이용하여 동영상으로 부터 손글씨를 검출한다.  \n",
    "모델 학습에는 두 가지 방법을 사용한다.  \n",
    "\n",
    "1. MNIST 데이터베이스를 이용하여 모델을 지도\n",
    "2. 레이블 되어진 동영상을 이용하여 모델을 지도 (직접 획득한 데이터를 이용)\n",
    "  \n",
    "두 모델에 대한 탐구를 통하여 다음의 목표를 달성하고자 한다.\n",
    "\n",
    "* 영상처리를 이용한 실시간 분류기에 대한 접근 및 이해  \n",
    "* 향후 연구방향 제시"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 본론\n",
    "---\n",
    "\n",
    "## 목표: 동영상을 이용한 Machine Learning 지도학습 데이터 수집\n",
    "\n",
    "## 설계:\n",
    "본 프로그램의 구현 과정은 크게 3가지 단계로 이루어집니다.\n",
    "\n",
    "1. raw 데이터 획득\n",
    "    * 균일한 색상의 배경과 특정색상 선을 사용하여 배경과 검출하고자 하는 숫자를 명확하게 함. (화이트보드/붉은색 마커 사용)\n",
    "    * 녹화된 영상의 파일명에 해당 숫자 값을 명시하여 레이블함. (ex: `<정답>_<영상번호>.mov`)\n",
    "\n",
    "2. 데이터셋 준비\n",
    "    * OpenCV를 이용하여 데이터를 28x28, Single-channel Image로 통일시킴\n",
    "    * 데이터에 올바른 레이블을 매칭시켜줌.\n",
    "\n",
    "3. 머신러닝 모델에 적용\n",
    "    * TensorFlow 시스템을 이용하여 모델 구현, 학습, 그리고 검증"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "    \n",
    "## 1. raw 데이터 획득\n",
    "\n",
    "가장 먼저 해야할 일은 역시 데이터 수집입니다.  \n",
    "힘들지만 손수 글씨를 써가며 촬영하는 방식으로 데이터를 획득하였습니다.\n",
    "\n",
    "촬영한 영상 원본은 다음 링크에서 확인하실 수 있습니다.\n",
    "\n",
    "* Google Drive URL: https://drive.google.com/drive/folders/1-HyJBGEiGAk_lxCEdQ6mXYBZVoAfACZt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. 데이터셋 준비\n",
    "\n",
    "### 2.1 데이터 정규화\n",
    "\n",
    "다음의 코드를 이용하여 정규화(: 화면 자르기, 손글씨 검출, 노이즈제거, 차원축소, 크기축소) 작업을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# this code normalizes raw video files into 28x28 single-channel images\n",
    "# \n",
    "# (C) 2020 Kim Dong Joo, Dongguk University, Gyeongju\n",
    "# email hepheir@gmail.com\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "RAW_DATA_PATH = './resources/raw/'\n",
    "OUT_DATA_PATH = './resources/out/'\n",
    "\n",
    "def main():\n",
    "    ls = os.listdir(RAW_DATA_PATH)\n",
    "\n",
    "    usr_sel = input(\"%d files found.\\nproceed? [y/n]: \" % len(ls))\n",
    "    if usr_sel != 'y':\n",
    "        print(\"Canceled by user.\")\n",
    "        return\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "\n",
    "    for file_index in range(len(ls)):\n",
    "        filename = ls[file_index]\n",
    "\n",
    "        if filename.startswith('TEST'): continue\n",
    "\n",
    "        video = cv2.VideoCapture(RAW_DATA_PATH + filename)\n",
    "        frame_index = 0\n",
    "        \n",
    "        while video.isOpened():\n",
    "            ret, frame = video.read()\n",
    "            if not ret: break\n",
    "\n",
    "            # -----------------------------------------------------------\n",
    "            # Cut & Resize\n",
    "            #   * why [:,420:-420]?\n",
    "            #   --> frame.shape of the used videos are 1920x1080.\n",
    "            #       to make it square, sliced 'x' from each frames from 420 to -420\n",
    "            #       (1080, 1920, 3) -> (1080, 1080, 3)\n",
    "            #   * why resize to (488,488)?\n",
    "            #   --> to fasten the process.\n",
    "            #       chose 448, since it is 28 multiplied by 2^4\n",
    "            #       (448=28*(2^4))\n",
    "            # -----------------------------------------------------------\n",
    "            frame = frame[::-1,-420:420:-1] \n",
    "            frame = cv2.resize(frame, (448,448))\n",
    "\n",
    "            # -----------------------------------------------------------\n",
    "            # Detect hand writings\n",
    "            # -----------------------------------------------------------\n",
    "            yuv_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2YUV)\n",
    "            red_yuv_range = (( 64,  0,128),\n",
    "                             (255,128,255))\n",
    "            red_yuv_mask = cv2.inRange(yuv_frame, red_yuv_range[0], red_yuv_range[1])\n",
    "\n",
    "            hsv_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "            red_hsv_range = ((  0, 64, 64),\n",
    "                             (255,255,255))\n",
    "            red_hsv_mask = cv2.inRange(hsv_frame, red_hsv_range[0], red_hsv_range[1])\n",
    "            \n",
    "            red_mask = cv2.bitwise_and(red_yuv_mask, red_hsv_mask)\n",
    "\n",
    "            # -----------------------------------------------------------\n",
    "            # Dilate mask to make them survive from resizing\n",
    "            # -----------------------------------------------------------\n",
    "            red_mask = cv2.dilate(red_mask,\n",
    "                                  cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (9,9)),\n",
    "                                  iterations=3)\n",
    "\n",
    "            # -----------------------------------------------------------\n",
    "            # Resize and export frames\n",
    "            # -----------------------------------------------------------\n",
    "            out_frame = cv2.resize(red_mask, (28, 28))\n",
    "            out_filename = OUT_DATA_PATH + filename.replace('.MOV', '_%d.png' % frame_index)\n",
    "\n",
    "            cv2.imwrite(out_filename, out_frame)\n",
    "            frame_index += 1\n",
    "\n",
    "            print(\"\\rCreating (%d/%d)... %s         \" % (file_index+1, len(ls), out_filename), end=\"\")\n",
    "        video.release()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 레이블 매칭\n",
    "\n",
    "다음은 벤치마킹할 mnist의 손글씨 검출 학습데이터입니다.\n",
    "이 예제를 통해 데이터셋이 어떠한 형태로 저장되는지 알아볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import cv2\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "print('x_train[0]\\n', x_train[0])\n",
    "print('y_train[0]\\n', y_train[0])\n",
    "\n",
    "cv2.imshow(y_train[0], x_train[0])\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞선 예제를 통해 데이터셋이 어떠한 형태로 주어지는지를 보았습니다.\n",
    "`x_train`, `y_train`에는 각각 (28x28xN), (label x N)의 형태로 데이터와 레이블이 존재하며 1:1 대응을 이루게 됩니다.\n",
    "  \n",
    "이를 바탕으로 '2. 데이터 정규화'에서 획득한 이미지들의 레이블링을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "DATA_PATH = './resources/out/'\n",
    "\n",
    "x_train, y_train = [], []\n",
    "\n",
    "for filename in os.listdir(DATA_PATH):\n",
    "    if filename.startswith('TEST'): continue # 레이블링이 불가능한 영상은 제외\n",
    "\n",
    "    frame = cv2.imread(DATA_PATH + filename)\n",
    "    label = filename.split('_')[0]\n",
    "    \n",
    "    x_train.append(frame)\n",
    "    y_train.append(label)\n",
    "\n",
    "x_train = np.array(x_train, dtype=np.uint8)\n",
    "y_train = np.array(y_train, dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##  3. 실제 모델에 적용\n",
    "\n",
    "이번에는 mnist 데이터셋을 이용하여 학습시킨 모델에 사용자 데이터셋('3. 레이블 매칭'에서 생성)을 적용시켜 볼 것입니다.  \n",
    "\n",
    "### 3.1 mnist 데이터셋으로 학습시킨 모델\n",
    "우선은 mnist 데이터셋을 이용하여 모델을 학습시키고 결과를 적용합니다. [모델 저장과 복원 참고](https://www.tensorflow.org/tutorials/keras/save_and_load?hl=ko)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 60000 samples\nEpoch 1/5\n60000/60000 [==============================] - 5s 86us/sample - loss: 0.2949 - accuracy: 0.9141\nEpoch 2/5\n60000/60000 [==============================] - 5s 79us/sample - loss: 0.1415 - accuracy: 0.9578\nEpoch 3/5\n60000/60000 [==============================] - 6s 96us/sample - loss: 0.1078 - accuracy: 0.9671\nEpoch 4/5\n60000/60000 [==============================] - 6s 105us/sample - loss: 0.0898 - accuracy: 0.9717\nEpoch 5/5\n60000/60000 [==============================] - 5s 87us/sample - loss: 0.0748 - accuracy: 0.9764\n10000/10000 - 0s - loss: 0.0732 - accuracy: 0.9778\n"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "CHECKPOINTS_PATH = './resources/checkpoints/mnist'\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Dense(10, activation='softmax')\n",
    "        ])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "model.evaluate(x_test, y_test, verbose=2)\n",
    "\n",
    "model.save_weights(CHECKPOINTS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 저장한 모델을 불러오기\n",
    "다음 코드를 통해 모델이 제대로 저장되었는지 확인해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "3281/3281 - 0s - loss: 12.7142 - accuracy: 0.1439\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[12.71417108783995, 0.14385858]"
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from resources.models import mnist\n",
    "\n",
    "model = mnist.model # 3.1 에서 사용한 모델\n",
    "\n",
    "model.load_weights(mnist.checkpoint)\n",
    "model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 사용자 데이터셋으로 테스트\n",
    "모델이 제대로 저장되었음을 확인했으니, 모델에 준비해온 데이터셋을 적용해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "3281/3281 [==============================] - 0s 74us/sample - loss: 12.7142 - accuracy: 0.1439\n"
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from resources.models import mnist\n",
    "\n",
    "DATA_PATH = './resources/out/'\n",
    "\n",
    "def load_user_data():\n",
    "    x, y = [], []\n",
    "    for filename in os.listdir(DATA_PATH):\n",
    "        if filename.startswith('TEST'): continue\n",
    "        \n",
    "        frame = cv2.imread(DATA_PATH + filename, cv2.IMREAD_GRAYSCALE)\n",
    "        label = filename.split('_')[0]\n",
    "        \n",
    "        if type(frame) is type(None): continue # To sort out dummy files as '.DS_Store'\n",
    "        if not frame.any(): continue # Empty frame\n",
    "\n",
    "        x.append(frame.copy())\n",
    "        y.append(label)\n",
    "\n",
    "    x = np.array(x, dtype=np.uint8)\n",
    "    y = np.array(y, dtype=np.uint8)\n",
    "    return x/255.0, y\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    x_test, y_test = load_user_data()\n",
    "\n",
    "    model = mnist.model\n",
    "    model.load_weights(mnist.checkpoint)\n",
    "    \n",
    "    model.evaluate(x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정말 아쉽게도 14.39%의 정확성밖에 보이지 못하였습니다.  \n",
    "  \n",
    "아마도 사용자 데이터 중에서 글씨가 덜 써진, 혹은 기타 노이즈(ex. 글씨외에도 손이나 팔뚝이 검출)로 인하여 올바르게 레이블이 되어지지 않은 프레임들이 문제가 되지 않나 싶습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 실시간 손글씨 검출\n",
    "\n",
    "보다 직관적인 결과 확인을 위해, 동영상에서 실시간으로 손글씨 분류 결과를 확인 할 수 있는 모델을 생성해보기로 합니다.  \n",
    "위 모델에서 샘플당 처리소요시간은 74us로, 실시간 처리에는 큰 지장이 없으리라 생각이됩니다.\n",
    "\n",
    "### 4.1 카메라 이용 및 녹화\n",
    "\n",
    "우선은 카메라를 이용하는 방법과 영상을 녹화하는 방법에 대하여 간단히 훑어봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# demonstrate how to record a video with a camera using opencv-python\n",
    "# \n",
    "# (C) 2020 Kim Dong Joo, Dongguk University, Gyeongju\n",
    "# email hepheir@gmail.com\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "import cv2\n",
    "from datetime import datetime\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Video I/O Settings\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "FRAME_WIDTH, FRAME_HEIGHT = 256, 256\n",
    "FPS = 20.0\n",
    "\n",
    "FILE_NAME_PATTERN = \"./records/\"+\"base-%y%m%d_%H%M%S.avi\"\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Utilities\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "status = {\n",
    "    'frames': 0,\n",
    "    'video_length': 0}\n",
    "\n",
    "def updateStatus():\n",
    "    # Put all the values to be updated for each cycles\n",
    "    global status\n",
    "    status['frames'] += 1\n",
    "    status['video_length'] = status['frames'] // FPS\n",
    "\n",
    "def showStatus():\n",
    "    global status\n",
    "    msg = \"\\r\"\n",
    "    for key in status:\n",
    "        msg += \"%s: %s \" % (key, str(status[key]))\n",
    "    print(msg, end=\"\")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Main loop\n",
    "# -----------------------------------------------------------\n",
    "if __name__ == '__main__':\n",
    "    vidIn = cv2.VideoCapture(0)\n",
    "    vidIn.set(cv2.CAP_PROP_FRAME_WIDTH,  FRAME_WIDTH)\n",
    "    vidIn.set(cv2.CAP_PROP_FRAME_HEIGHT, FRAME_HEIGHT)\n",
    "\n",
    "    vidOut = cv2.VideoWriter(datetime.now().strftime(FILE_NAME_PATTERN),\n",
    "                             cv2.VideoWriter_fourcc(*'MJPG'),\n",
    "                             FPS,\n",
    "                             (FRAME_WIDTH, FRAME_HEIGHT))\n",
    "\n",
    "    while vidIn.isOpened():\n",
    "        ret, frame = vidIn.read()\n",
    "        if not ret: break\n",
    "\n",
    "        frame = cv2.resize(frame, (FRAME_HEIGHT, FRAME_WIDTH))\n",
    "        # -----------------------------------------------------------\n",
    "        # TODO\n",
    "        # -----------------------------------------------------------\n",
    "        cv2.imshow('cam', frame)\n",
    "        vidOut.write(frame)\n",
    "        \n",
    "        # -----------------------------------------------------------\n",
    "        # Key mappings\n",
    "        # -----------------------------------------------------------\n",
    "        key = cv2.waitKey(20) & 0xFF\n",
    "        if key == 27: break # ESC\n",
    "\n",
    "        # -----------------------------------------------------------\n",
    "        # Debugs\n",
    "        # -----------------------------------------------------------\n",
    "        updateStatus()\n",
    "        showStatus()\n",
    "\n",
    "    vidIn.release()\n",
    "    vidOut.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "print(\"\")\n",
    "print(\"Successfully Closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 4.2 실시간 예측 모델\n",
    "\n",
    "다음은 카메라 장치를 이용하여 영상을 받아오고, 실시간으로 손글씨를 검출, 정규화한다.  \n",
    "학습된 모델을 사용하여 정규화한 데이터의 값을 예측한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Error when checking input: expected conv2d_input to have 4 dimensions, but got array with shape (1, 28, 28)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-163be5cd941b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     64\u001b[0m                               iterations=3)\n\u001b[1;32m     65\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mred_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m   1014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m         workers=workers, use_multiprocessing=use_multiprocessing, **kwargs)\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[0;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    424\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    427\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m       \u001b[0muse_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_samples\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    644\u001b[0m     \u001b[0mstandardize_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m     x, y, sample_weights = standardize(\n\u001b[0;32m--> 646\u001b[0;31m         x, y, sample_weight=sample_weights)\n\u001b[0m\u001b[1;32m    647\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0madapter_cls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mdata_adapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mListsOfScalarsDataAdapter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0mstandardize_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstandardize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2381\u001b[0m         \u001b[0mis_dataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2382\u001b[0m         \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2383\u001b[0;31m         batch_size=batch_size)\n\u001b[0m\u001b[1;32m   2384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2385\u001b[0m   def _standardize_tensors(self, x, y, sample_weight, run_eagerly, dict_inputs,\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_tensors\u001b[0;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[1;32m   2408\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2409\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2410\u001b[0;31m           exception_prefix='input')\n\u001b[0m\u001b[1;32m   2411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2412\u001b[0m     \u001b[0;31m# Get typespecs for the input data and sanitize it if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    571\u001b[0m                            \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m                            \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m                            'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    574\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m           \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected conv2d_input to have 4 dimensions, but got array with shape (1, 28, 28)"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------\n",
    "# demonstrate how to classify hand writings from a video stream\n",
    "# being captured in real-time, using opencv-python\n",
    "# \n",
    "# (C) 2020 Kim Dong Joo, Dongguk University, Gyeongju\n",
    "# email hepheir@gmail.com\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from resources.models import mnist_3_lyr\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Main loop\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "def writeText_1(frame, txt):\n",
    "    cv2.putText(frame,\n",
    "                txt,\n",
    "                (8, FRAME_HEIGHT-32),\n",
    "                cv2.FONT_HERSHEY_PLAIN,\n",
    "                1.2,\n",
    "                (128, 128, 255),\n",
    "                lineType=cv2.LINE_AA)\n",
    "def writeText_2(frame, txt):\n",
    "    cv2.putText(frame,\n",
    "                txt,\n",
    "                (8, FRAME_HEIGHT-16),\n",
    "                cv2.FONT_HERSHEY_PLAIN,\n",
    "                1.2,\n",
    "                (128, 128, 255),\n",
    "                lineType=cv2.LINE_AA)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = mnist_3_lyr.model\n",
    "    model.load_weights(mnist_3_lyr.checkpoint)\n",
    "\n",
    "    FRAME_WIDTH, FRAME_HEIGHT = 448, 448\n",
    "    vidIn = cv2.VideoCapture(0)\n",
    "\n",
    "    while vidIn.isOpened():\n",
    "        ret, frame = vidIn.read()\n",
    "        if not ret: break\n",
    "\n",
    "        frame = frame[:,420:-420] # frame[::-1,-420:420:-1]\n",
    "        frame = cv2.resize(frame, (FRAME_HEIGHT, FRAME_WIDTH))\n",
    "\n",
    "        # -----------------------------------------------------------\n",
    "        # Detect hand writings & normalize\n",
    "        # -----------------------------------------------------------\n",
    "        yuv_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2YUV)\n",
    "        red_yuv_mask = cv2.inRange(yuv_frame,\n",
    "                                   ( 64,  0,128),\n",
    "                                   (255,128,255))\n",
    "\n",
    "        hsv_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "        red_hsv_mask = cv2.inRange(hsv_frame,\n",
    "                                   (  0, 48, 48),\n",
    "                                   (255,255,255))\n",
    "        red_mask = cv2.dilate(cv2.bitwise_and(red_yuv_mask, red_hsv_mask),\n",
    "                              cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (9,9)),\n",
    "                              iterations=3)\n",
    "        x = cv2.resize(red_mask, (28, 28))\n",
    "        p = model.predict(np.array([x]))\n",
    "        print(p)\n",
    "        y = np.where(p == 1)[0][0]\n",
    "\n",
    "        \n",
    "        # -----------------------------------------------------------\n",
    "        # Key mappings\n",
    "        # -----------------------------------------------------------\n",
    "        key = cv2.waitKey(20) & 0xFF\n",
    "        status['key'] = chr(key)\n",
    "\n",
    "        if key == 27: break # ESC\n",
    "\n",
    "        y_train = None\n",
    "        for n in range(10):\n",
    "            if key == ord(str(n)):\n",
    "                y_train = n\n",
    "                break\n",
    "        if not (y_train is None): # 레이블이 주어지면 학습\n",
    "            model.fit(np.array([x]), np.array([y_train]), epochs=1)\n",
    "            writeText_1(frame, \"Label Corrected to : %d\" % y_train)\n",
    "        # -----------------------------------------------------------\n",
    "        # Result\n",
    "        # -----------------------------------------------------------\n",
    "        writeText_2(frame, \"Predict: %d\" % y)\n",
    "\n",
    "        cv2.imshow('cam', frame)\n",
    "        cv2.imshow('mask', x)\n",
    "\n",
    "    vidIn.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "print(\"\")\n",
    "print(\"Successfully Closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# this code normalizes raw video files into 28x28 single-channel images\n",
    "# \n",
    "# (C) 2020 Kim Dong Joo, Dongguk University, Gyeongju\n",
    "# email hepheir@gmail.com\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "RAW_DATA_PATH = './resources/raw/'\n",
    "OUT_DATA_PATH = './resources/out/'\n",
    "\n",
    "def main():\n",
    "    ls = os.listdir(RAW_DATA_PATH)\n",
    "\n",
    "    usr_sel = input(\"%d files found.\\nproceed? [y/n]: \" % len(ls))\n",
    "    if usr_sel != 'y':\n",
    "        print(\"Canceled by user.\")\n",
    "        return\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "\n",
    "    for file_index in range(len(ls)):\n",
    "        filename = ls[file_index]\n",
    "\n",
    "        if filename.startswith('TEST'): continue\n",
    "\n",
    "        video = cv2.VideoCapture(RAW_DATA_PATH + filename)\n",
    "        frame_index = 0\n",
    "        \n",
    "        while video.isOpened():\n",
    "            ret, frame = video.read()\n",
    "            if not ret: break\n",
    "\n",
    "            # -----------------------------------------------------------\n",
    "            # Cut & Resize\n",
    "            #   * why [:,420:-420]?\n",
    "            #   --> frame.shape of the used videos are 1920x1080.\n",
    "            #       to make it square, sliced 'x' from each frames from 420 to -420\n",
    "            #       (1080, 1920, 3) -> (1080, 1080, 3)\n",
    "            #   * why resize to (488,488)?\n",
    "            #   --> to fasten the process.\n",
    "            #       chose 448, since it is 28 multiplied by 2^4\n",
    "            #       (448=28*(2^4))\n",
    "            # -----------------------------------------------------------\n",
    "            frame = frame[::-1,-420:420:-1] \n",
    "            frame = cv2.resize(frame, (448,448))\n",
    "\n",
    "            # -----------------------------------------------------------\n",
    "            # Detect hand writings\n",
    "            # -----------------------------------------------------------\n",
    "            yuv_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2YUV)\n",
    "            red_yuv_range = (( 64,  0,128),\n",
    "                             (255,128,255))\n",
    "            red_yuv_mask = cv2.inRange(yuv_frame, red_yuv_range[0], red_yuv_range[1])\n",
    "\n",
    "            hsv_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "            red_hsv_range = ((  0, 64, 64),\n",
    "                             (255,255,255))\n",
    "            red_hsv_mask = cv2.inRange(hsv_frame, red_hsv_range[0], red_hsv_range[1])\n",
    "            \n",
    "            red_mask = cv2.bitwise_and(red_yuv_mask, red_hsv_mask)\n",
    "\n",
    "            # -----------------------------------------------------------\n",
    "            # Dilate mask to make them survive from resizing\n",
    "            # -----------------------------------------------------------\n",
    "            red_mask = cv2.dilate(red_mask,\n",
    "                                  cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (9,9)),\n",
    "                                  iterations=3)\n",
    "\n",
    "            # -----------------------------------------------------------\n",
    "            # Resize and export frames\n",
    "            # -----------------------------------------------------------\n",
    "            out_frame = cv2.resize(red_mask, (28, 28))\n",
    "            out_filename = OUT_DATA_PATH + filename.replace('.MOV', '_%d.png' % frame_index)\n",
    "\n",
    "            cv2.imwrite(out_filename, out_frame)\n",
    "            frame_index += 1\n",
    "\n",
    "            print(\"\\rCreating (%d/%d)... %s         \" % (file_index+1, len(ls), out_filename), end=\"\")\n",
    "        video.release()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# 결론\n",
    "\n",
    "## 해석\n",
    "## 고찰\n",
    "## 계획\n",
    "\n",
    "---\n",
    "\n",
    "# 부록\n",
    "---\n",
    "## 참고\n",
    "* Teachable Machine: 심태섭 연구원이 소개해준 온라인 머신러닝 서비스로, 본 활동에 영감을 받음\n",
    "* "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('ml': conda)",
   "language": "python",
   "name": "python37764bitmlconda56395923f6734d2e9d871b41acf277cc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}