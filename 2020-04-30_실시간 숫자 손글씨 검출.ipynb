{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 서론\n",
    "---\n",
    "\n",
    "## 주제: 실시간 숫자 손글씨 검출\n",
    "참여 연구자: 김동주, 이도경\n",
    "\n",
    "## 요약:\n",
    "OpenCV를 이용하여 동영상으로 부터 손글씨를 검출한다.  \n",
    "모델 학습에는 두 가지 방법을 사용한다.  \n",
    "\n",
    "1. MNIST 데이터베이스를 이용하여 모델을 지도\n",
    "2. 레이블 되어진 동영상을 이용하여 모델을 지도 (직접 획득한 데이터를 이용)\n",
    "  \n",
    "두 모델에 대한 탐구를 통하여 다음의 목표를 달성하고자 한다.\n",
    "\n",
    "* 영상처리를 이용한 실시간 분류기에 대한 접근 및 이해  \n",
    "* 향후 연구방향 제시"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 본론\n",
    "---\n",
    "\n",
    "## 목표: 동영상을 이용한 Machine Learning 지도학습 데이터 수집\n",
    "\n",
    "## 설계:\n",
    "본 프로그램의 구현 과정은 크게 3가지 단계로 이루어집니다.\n",
    "\n",
    "1. raw 데이터 획득\n",
    "    * 균일한 색상의 배경과 특정색상 선을 사용하여 배경과 검출하고자 하는 숫자를 명확하게 함. (화이트보드/붉은색 마커 사용)\n",
    "    * 녹화된 영상의 파일명에 해당 숫자 값을 명시하여 레이블함. (ex: `<정답>_<영상번호>.mov`)\n",
    "\n",
    "2. 데이터셋 준비\n",
    "    * OpenCV를 이용하여 데이터를 28x28, Single-channel Image로 통일시킴\n",
    "    * 데이터에 올바른 레이블을 매칭시켜줌.\n",
    "\n",
    "3. 머신러닝 모델에 적용\n",
    "    * TensorFlow 시스템을 이용하여 모델 구현, 학습, 그리고 검증"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "    \n",
    "## 1. raw 데이터 획득\n",
    "\n",
    "가장 먼저 해야할 일은 역시 데이터 수집입니다.  \n",
    "힘들지만 손수 글씨를 써가며 촬영하는 방식으로 데이터를 획득하였습니다.\n",
    "\n",
    "촬영한 영상 원본은 다음 링크에서 확인하실 수 있습니다.\n",
    "\n",
    "* Google Drive URL: https://drive.google.com/drive/folders/1-HyJBGEiGAk_lxCEdQ6mXYBZVoAfACZt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. 데이터셋 준비\n",
    "\n",
    "### 2.1 데이터 정규화\n",
    "\n",
    "다음의 코드를 이용하여 정규화(: 화면 자르기, 손글씨 검출, 노이즈제거, 차원축소, 크기축소) 작업을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# this code normalizes raw video files into 28x28 single-channel images\n",
    "# \n",
    "# (C) 2020 Kim Dong Joo, Dongguk University, Gyeongju\n",
    "# email hepheir@gmail.com\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "RAW_DATA_PATH = './resources/raw/'\n",
    "OUT_DATA_PATH = './resources/out/'\n",
    "\n",
    "def main():\n",
    "    ls = os.listdir(RAW_DATA_PATH)\n",
    "\n",
    "    usr_sel = input(\"%d files found.\\nproceed? [y/n]: \" % len(ls))\n",
    "    if usr_sel != 'y':\n",
    "        print(\"Canceled by user.\")\n",
    "        return\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "\n",
    "    for file_index in range(len(ls)):\n",
    "        filename = ls[file_index]\n",
    "\n",
    "        if filename.startswith('TEST'): continue\n",
    "\n",
    "        video = cv2.VideoCapture(RAW_DATA_PATH + filename)\n",
    "        frame_index = 0\n",
    "        \n",
    "        while video.isOpened():\n",
    "            ret, frame = video.read()\n",
    "            if not ret: break\n",
    "\n",
    "            # -----------------------------------------------------------\n",
    "            # Cut & Resize\n",
    "            #   * why [:,420:-420]?\n",
    "            #   --> frame.shape of the used videos are 1920x1080.\n",
    "            #       to make it square, sliced 'x' from each frames from 420 to -420\n",
    "            #       (1080, 1920, 3) -> (1080, 1080, 3)\n",
    "            #   * why resize to (488,488)?\n",
    "            #   --> to fasten the process.\n",
    "            #       chose 448, since it is 28 multiplied by 2^4\n",
    "            #       (448=28*(2^4))\n",
    "            # -----------------------------------------------------------\n",
    "            frame = frame[::-1,-420:420:-1] \n",
    "            frame = cv2.resize(frame, (448,448))\n",
    "\n",
    "            # -----------------------------------------------------------\n",
    "            # Detect hand writings\n",
    "            # -----------------------------------------------------------\n",
    "            yuv_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2YUV)\n",
    "            red_yuv_range = (( 64,  0,128),\n",
    "                             (255,128,255))\n",
    "            red_yuv_mask = cv2.inRange(yuv_frame, red_yuv_range[0], red_yuv_range[1])\n",
    "\n",
    "            hsv_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "            red_hsv_range = ((  0, 64, 64),\n",
    "                             (255,255,255))\n",
    "            red_hsv_mask = cv2.inRange(hsv_frame, red_hsv_range[0], red_hsv_range[1])\n",
    "            \n",
    "            red_mask = cv2.bitwise_and(red_yuv_mask, red_hsv_mask)\n",
    "\n",
    "            # -----------------------------------------------------------\n",
    "            # Dilate mask to make them survive from resizing\n",
    "            # -----------------------------------------------------------\n",
    "            red_mask = cv2.dilate(red_mask,\n",
    "                                  cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (9,9)),\n",
    "                                  iterations=3)\n",
    "\n",
    "            # -----------------------------------------------------------\n",
    "            # Resize and export frames\n",
    "            # -----------------------------------------------------------\n",
    "            out_frame = cv2.resize(red_mask, (28, 28))\n",
    "            out_filename = OUT_DATA_PATH + filename.replace('.MOV', '_%d.png' % frame_index)\n",
    "\n",
    "            cv2.imwrite(out_filename, out_frame)\n",
    "            frame_index += 1\n",
    "\n",
    "            print(\"\\rCreating (%d/%d)... %s         \" % (file_index+1, len(ls), out_filename), end=\"\")\n",
    "        video.release()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 레이블 매칭\n",
    "\n",
    "다음은 벤치마킹할 mnist의 손글씨 검출 학습데이터입니다.\n",
    "이 예제를 통해 데이터셋이 어떠한 형태로 저장되는지 알아볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import cv2\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "print('x_train[0]\\n', x_train[0])\n",
    "print('y_train[0]\\n', y_train[0])\n",
    "\n",
    "cv2.imshow(y_train[0], x_train[0])\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞선 예제를 통해 데이터셋이 어떠한 형태로 주어지는지를 보았습니다.\n",
    "`x_train`, `y_train`에는 각각 (28x28xN), (label x N)의 형태로 데이터와 레이블이 존재하며 1:1 대응을 이루게 됩니다.\n",
    "  \n",
    "이를 바탕으로 '2. 데이터 정규화'에서 획득한 이미지들의 레이블링을 수행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "DATA_PATH = './resources/out/'\n",
    "\n",
    "x_train, y_train = [], []\n",
    "\n",
    "for filename in os.listdir(DATA_PATH):\n",
    "    if filename.startswith('TEST'): continue # 레이블링이 불가능한 영상은 제외\n",
    "\n",
    "    frame = cv2.imread(DATA_PATH + filename)\n",
    "    label = filename.split('_')[0]\n",
    "    \n",
    "    x_train.append(frame)\n",
    "    y_train.append(label)\n",
    "\n",
    "x_train = np.array(x_train, dtype=np.uint8)\n",
    "y_train = np.array(y_train, dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##  3. 실제 모델에 적용\n",
    "\n",
    "이번에는 mnist 데이터셋을 이용하여 학습시킨 모델에 사용자 데이터셋('3. 레이블 매칭'에서 생성)을 적용시켜 볼 것입니다.  \n",
    "\n",
    "### 3.1 mnist 데이터셋으로 학습시킨 모델\n",
    "우선은 mnist 데이터셋을 이용하여 모델을 학습시키고 결과를 적용합니다. [모델 저장과 복원 참고](https://www.tensorflow.org/tutorials/keras/save_and_load?hl=ko)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 60000 samples\nEpoch 1/5\n60000/60000 [==============================] - 5s 86us/sample - loss: 0.2949 - accuracy: 0.9141\nEpoch 2/5\n60000/60000 [==============================] - 5s 79us/sample - loss: 0.1415 - accuracy: 0.9578\nEpoch 3/5\n60000/60000 [==============================] - 6s 96us/sample - loss: 0.1078 - accuracy: 0.9671\nEpoch 4/5\n60000/60000 [==============================] - 6s 105us/sample - loss: 0.0898 - accuracy: 0.9717\nEpoch 5/5\n60000/60000 [==============================] - 5s 87us/sample - loss: 0.0748 - accuracy: 0.9764\n10000/10000 - 0s - loss: 0.0732 - accuracy: 0.9778\n"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "CHECKPOINTS_PATH = './resources/checkpoints/mnist'\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Dense(10, activation='softmax')\n",
    "        ])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "model.evaluate(x_test, y_test, verbose=2)\n",
    "\n",
    "model.save_weights(CHECKPOINTS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 저장한 모델을 불러오기\n",
    "다음 코드를 통해 모델이 제대로 저장되었는지 확인해봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "3281/3281 - 0s - loss: 12.7142 - accuracy: 0.1439\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[12.71417108783995, 0.14385858]"
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from resources.models import mnist\n",
    "\n",
    "model = mnist.model # 3.1 에서 사용한 모델\n",
    "\n",
    "model.load_weights(mnist.checkpoint)\n",
    "model.evaluate(x_test, y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 사용자 데이터셋으로 테스트\n",
    "모델이 제대로 저장되었음을 확인했으니, 모델에 준비해온 데이터셋을 적용해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "3281/3281 [==============================] - 0s 74us/sample - loss: 12.7142 - accuracy: 0.1439\n"
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from resources.models import mnist\n",
    "\n",
    "DATA_PATH = './resources/out/'\n",
    "\n",
    "def load_user_data():\n",
    "    x, y = [], []\n",
    "    for filename in os.listdir(DATA_PATH):\n",
    "        if filename.startswith('TEST'): continue\n",
    "        \n",
    "        frame = cv2.imread(DATA_PATH + filename, cv2.IMREAD_GRAYSCALE)\n",
    "        label = filename.split('_')[0]\n",
    "        \n",
    "        if type(frame) is type(None): continue # To sort out dummy files as '.DS_Store'\n",
    "        if not frame.any(): continue # Empty frame\n",
    "\n",
    "        x.append(frame.copy())\n",
    "        y.append(label)\n",
    "\n",
    "    x = np.array(x, dtype=np.uint8)\n",
    "    y = np.array(y, dtype=np.uint8)\n",
    "    return x/255.0, y\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    x_test, y_test = load_user_data()\n",
    "\n",
    "    model = mnist.model\n",
    "    model.load_weights(mnist.checkpoint)\n",
    "    \n",
    "    model.evaluate(x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정말 아쉽게도 14.39%의 정확성밖에 보이지 못하였습니다.  \n",
    "  \n",
    "아마도 사용자 데이터 중에서 글씨가 덜 써진, 혹은 기타 노이즈(ex. 글씨외에도 손이나 팔뚝이 검출)로 인하여 올바르게 레이블이 되어지지 않은 프레임들이 문제가 되지 않나 싶습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 실시간 손글씨 검출\n",
    "\n",
    "보다 직관적인 결과 확인을 위해, 동영상에서 실시간으로 손글씨 분류 결과를 확인 할 수 있는 모델을 생성해보기로 합니다.  \n",
    "위 모델에서 샘플당 처리소요시간은 74us로, 실시간 처리에는 큰 지장이 없으리라 생각이됩니다.\n",
    "\n",
    "### 4.1 카메라 이용 및 녹화\n",
    "\n",
    "우선은 카메라를 이용하는 방법과 영상을 녹화하는 방법에 대하여 간단히 훑어본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# demonstrate how to record a video with a camera using opencv-python\n",
    "# \n",
    "# (C) 2020 Kim Dong Joo, Dongguk University, Gyeongju\n",
    "# email hepheir@gmail.com\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "import cv2\n",
    "from datetime import datetime\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Video I/O Settings\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "FRAME_WIDTH, FRAME_HEIGHT = 256, 256\n",
    "FPS = 20.0\n",
    "\n",
    "FILE_NAME_PATTERN = \"./records/\"+\"base-%y%m%d_%H%M%S.avi\"\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Utilities\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "status = {\n",
    "    'frames': 0,\n",
    "    'video_length': 0}\n",
    "\n",
    "def updateStatus():\n",
    "    # Put all the values to be updated for each cycles\n",
    "    global status\n",
    "    status['frames'] += 1\n",
    "    status['video_length'] = status['frames'] // FPS\n",
    "\n",
    "def showStatus():\n",
    "    global status\n",
    "    msg = \"\\r\"\n",
    "    for key in status:\n",
    "        msg += \"%s: %s \" % (key, str(status[key]))\n",
    "    print(msg, end=\"\")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Main loop\n",
    "# -----------------------------------------------------------\n",
    "if __name__ == '__main__':\n",
    "    vidIn = cv2.VideoCapture(0)\n",
    "    vidIn.set(cv2.CAP_PROP_FRAME_WIDTH,  FRAME_WIDTH)\n",
    "    vidIn.set(cv2.CAP_PROP_FRAME_HEIGHT, FRAME_HEIGHT)\n",
    "\n",
    "    vidOut = cv2.VideoWriter(datetime.now().strftime(FILE_NAME_PATTERN),\n",
    "                             cv2.VideoWriter_fourcc(*'MJPG'),\n",
    "                             FPS,\n",
    "                             (FRAME_WIDTH, FRAME_HEIGHT))\n",
    "\n",
    "    while vidIn.isOpened():\n",
    "        ret, frame = vidIn.read()\n",
    "        if not ret: break\n",
    "\n",
    "        frame = cv2.resize(frame, (FRAME_HEIGHT, FRAME_WIDTH))\n",
    "        # -----------------------------------------------------------\n",
    "        # TODO\n",
    "        # -----------------------------------------------------------\n",
    "        cv2.imshow('cam', frame)\n",
    "        vidOut.write(frame)\n",
    "        \n",
    "        # -----------------------------------------------------------\n",
    "        # Key mappings\n",
    "        # -----------------------------------------------------------\n",
    "        key = cv2.waitKey(20) & 0xFF\n",
    "        if key == 27: break # ESC\n",
    "\n",
    "        # -----------------------------------------------------------\n",
    "        # Debugs\n",
    "        # -----------------------------------------------------------\n",
    "        updateStatus()\n",
    "        showStatus()\n",
    "\n",
    "    vidIn.release()\n",
    "    vidOut.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "print(\"\")\n",
    "print(\"Successfully Closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 4.2 베이스 코드\n",
    "\n",
    "다음은 카메라 장치를 이용하여 영상을 녹화하는 코드이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": [
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend",
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:Inconsistent references when loading the checkpoint into this object graph. Either the Trackable object references in the Python program have changed in an incompatible way, or the checkpoint was generated in an incompatible program.\n\nTwo checkpoint references resolved to different objects (<tensorflow.python.keras.layers.convolutional.Conv2D object at 0x64efc3510> and <tensorflow.python.keras.layers.pooling.MaxPooling2D object at 0x6542bf890>).\n"
    },
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Shapes (32,) and (128,) are incompatible",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-2e9f5973b695>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m               \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sparse_categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m               metrics=['accuracy'])\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCHECKPOINTS_PATH\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'mnist'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# -----------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch)\u001b[0m\n\u001b[1;32m    232\u001b[0m         raise ValueError('Load weights is not yet supported with TPUStrategy '\n\u001b[1;32m    233\u001b[0m                          'with steps_per_run greater than 1.')\n\u001b[0;32m--> 234\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_mismatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mtrackable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_automatic_dependency_tracking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/network.py\u001b[0m in \u001b[0;36mload_weights\u001b[0;34m(self, filepath, by_name, skip_mismatch)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0msave_format\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'h5'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msave_format\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tf'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m       \u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trackable_saver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mby_name\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m         raise NotImplementedError(\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/util.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, save_path)\u001b[0m\n\u001b[1;32m   1281\u001b[0m         graph_view=self._graph_view)\n\u001b[1;32m   1282\u001b[0m     base.CheckpointPosition(\n\u001b[0;32m-> 1283\u001b[0;31m         checkpoint=checkpoint, proto_id=0).restore(self._graph_view.root)\n\u001b[0m\u001b[1;32m   1284\u001b[0m     load_status = CheckpointLoadStatus(\n\u001b[1;32m   1285\u001b[0m         \u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, trackable)\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;31m# This object's correspondence with a checkpointed object is new, so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;31m# process deferred restorations for it and its dependencies.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mrestore_ops\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrackable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_restore_from_checkpoint_position\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrestore_ops\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_checkpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_restore_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrestore_ops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_restore_from_checkpoint_position\u001b[0;34m(self, checkpoint_position)\u001b[0m\n\u001b[1;32m    906\u001b[0m     restore_ops.extend(\n\u001b[1;32m    907\u001b[0m         current_position.checkpoint.restore_saveables(\n\u001b[0;32m--> 908\u001b[0;31m             tensor_saveables, python_saveables))\n\u001b[0m\u001b[1;32m    909\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrestore_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/util.py\u001b[0m in \u001b[0;36mrestore_saveables\u001b[0;34m(self, tensor_saveables, python_saveables)\u001b[0m\n\u001b[1;32m    287\u001b[0m              \"expecting %s\") % (tensor_saveables.keys(), validated_names))\n\u001b[1;32m    288\u001b[0m       new_restore_ops = functional_saver.MultiDeviceSaver(\n\u001b[0;32m--> 289\u001b[0;31m           validated_saveables).restore(self.save_path_tensor)\n\u001b[0m\u001b[1;32m    290\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestore_op\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_restore_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml/lib/python3.7/site-packages/tensorflow_core/python/training/saving/functional_saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, file_prefix)\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaver\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_single_device_savers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mrestore_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrestore_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml/lib/python3.7/site-packages/tensorflow_core/python/training/saving/functional_saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, file_prefix)\u001b[0m\n\u001b[1;32m    100\u001b[0m                                           structured_restored_tensors):\n\u001b[1;32m    101\u001b[0m       restore_ops[saveable.name] = saveable.restore(\n\u001b[0;32m--> 102\u001b[0;31m           restored_tensors, restored_shapes=None)\n\u001b[0m\u001b[1;32m    103\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrestore_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml/lib/python3.7/site-packages/tensorflow_core/python/training/saving/saveable_object_util.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, restored_tensors, restored_shapes)\u001b[0m\n\u001b[1;32m    114\u001b[0m       \u001b[0mrestored_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrestored_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m       return resource_variable_ops.shape_safe_assign_variable_handle(\n\u001b[0;32m--> 116\u001b[0;31m           self.handle_op, self._var_shape, restored_tensor)\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36mshape_safe_assign_variable_handle\u001b[0;34m(handle, shape, value, name)\u001b[0m\n\u001b[1;32m    295\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0m_handle_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0mvalue_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m   \u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_is_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m   return gen_resource_variable_ops.assign_variable_op(handle,\n\u001b[1;32m    299\u001b[0m                                                       \u001b[0mvalue_tensor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/ml/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36massert_is_compatible_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \"\"\"\n\u001b[1;32m   1109\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1110\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Shapes %s and %s are incompatible\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mmost_specific_compatible_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Shapes (32,) and (128,) are incompatible"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------\n",
    "# demonstrate how to classify hand writings from a video stream\n",
    "# being captured in real-time, using opencv-python\n",
    "# \n",
    "# (C) 2020 Kim Dong Joo, Dongguk University, Gyeongju\n",
    "# email hepheir@gmail.com\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Import model\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "from resources.models import mnist_3_lyr\n",
    "\n",
    "model = mnist_3_lyr.model\n",
    "model.load_weights(mnist_3_lyr.checkpoint)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Main loop\n",
    "# -----------------------------------------------------------\n",
    "if __name__ == '__main__':\n",
    "    FRAME_WIDTH, FRAME_HEIGHT = 448, 448\n",
    "    vidIn = cv2.VideoCapture(0)\n",
    "\n",
    "    while vidIn.isOpened():\n",
    "        ret, frame = vidIn.read()\n",
    "        if not ret: break\n",
    "\n",
    "        frame = frame[:,420:-420] # frame[::-1,-420:420:-1]\n",
    "        frame = cv2.resize(frame, (FRAME_HEIGHT, FRAME_WIDTH))\n",
    "\n",
    "        # -----------------------------------------------------------\n",
    "        # Detect hand writings & normalize\n",
    "        # -----------------------------------------------------------\n",
    "        yuv_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2YUV)\n",
    "        red_yuv_mask = cv2.inRange(yuv_frame,\n",
    "                                   ( 64,  0,128),\n",
    "                                   (255,128,255))\n",
    "\n",
    "        hsv_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "        red_hsv_mask = cv2.inRange(hsv_frame,\n",
    "                                   (  0, 48, 48),\n",
    "                                   (255,255,255))\n",
    "        red_mask = cv2.dilate(cv2.bitwise_and(red_yuv_mask, red_hsv_mask),\n",
    "                              cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (9,9)),\n",
    "                              iterations=3)\n",
    "        x = cv2.resize(red_mask, (28, 28))\n",
    "        y = np.where(model.predict(np.array([x])) == 1)[0]\n",
    "\n",
    "        \n",
    "        # -----------------------------------------------------------\n",
    "        # Key mappings\n",
    "        # -----------------------------------------------------------\n",
    "        key = cv2.waitKey(20) & 0xFF\n",
    "        status['key'] = chr(key)\n",
    "\n",
    "        if key == 27: break # ESC\n",
    "\n",
    "        y_train = None\n",
    "        for n in range(10):\n",
    "            if key == ord(str(n)):\n",
    "                y_train = n\n",
    "                break\n",
    "        if not (y_train is None): # 레이블이 주어지면 학습\n",
    "            model.fit(np.array([x]), np.array([y_train]), epochs=1)\n",
    "            cv2.putText(frame,\n",
    "                        \"Label Corrected to : %d\" % y_train,\n",
    "                        (8, FRAME_HEIGHT-32),\n",
    "                        cv2.FONT_HERSHEY_PLAIN,\n",
    "                        1.2,\n",
    "                        (128, 128, 255),\n",
    "                        lineType=cv2.LINE_AA)\n",
    "\n",
    "        # -----------------------------------------------------------\n",
    "        # Result\n",
    "        # -----------------------------------------------------------\n",
    "        cv2.putText(frame,\n",
    "                    \"Predict: %d\" % y,\n",
    "                    (8, FRAME_HEIGHT-8),\n",
    "                    cv2.FONT_HERSHEY_PLAIN,\n",
    "                    1.2,\n",
    "                    (128, 128, 255),\n",
    "                    lineType=cv2.LINE_AA)\n",
    "\n",
    "        cv2.imshow('cam', frame)\n",
    "        cv2.imshow('mask', x)\n",
    "\n",
    "    vidIn.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "print(\"\")\n",
    "print(\"Successfully Closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------\n",
    "# this code normalizes raw video files into 28x28 single-channel images\n",
    "# \n",
    "# (C) 2020 Kim Dong Joo, Dongguk University, Gyeongju\n",
    "# email hepheir@gmail.com\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "RAW_DATA_PATH = './resources/raw/'\n",
    "OUT_DATA_PATH = './resources/out/'\n",
    "\n",
    "def main():\n",
    "    ls = os.listdir(RAW_DATA_PATH)\n",
    "\n",
    "    usr_sel = input(\"%d files found.\\nproceed? [y/n]: \" % len(ls))\n",
    "    if usr_sel != 'y':\n",
    "        print(\"Canceled by user.\")\n",
    "        return\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "\n",
    "    for file_index in range(len(ls)):\n",
    "        filename = ls[file_index]\n",
    "\n",
    "        if filename.startswith('TEST'): continue\n",
    "\n",
    "        video = cv2.VideoCapture(RAW_DATA_PATH + filename)\n",
    "        frame_index = 0\n",
    "        \n",
    "        while video.isOpened():\n",
    "            ret, frame = video.read()\n",
    "            if not ret: break\n",
    "\n",
    "            # -----------------------------------------------------------\n",
    "            # Cut & Resize\n",
    "            #   * why [:,420:-420]?\n",
    "            #   --> frame.shape of the used videos are 1920x1080.\n",
    "            #       to make it square, sliced 'x' from each frames from 420 to -420\n",
    "            #       (1080, 1920, 3) -> (1080, 1080, 3)\n",
    "            #   * why resize to (488,488)?\n",
    "            #   --> to fasten the process.\n",
    "            #       chose 448, since it is 28 multiplied by 2^4\n",
    "            #       (448=28*(2^4))\n",
    "            # -----------------------------------------------------------\n",
    "            frame = frame[::-1,-420:420:-1] \n",
    "            frame = cv2.resize(frame, (448,448))\n",
    "\n",
    "            # -----------------------------------------------------------\n",
    "            # Detect hand writings\n",
    "            # -----------------------------------------------------------\n",
    "            yuv_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2YUV)\n",
    "            red_yuv_range = (( 64,  0,128),\n",
    "                             (255,128,255))\n",
    "            red_yuv_mask = cv2.inRange(yuv_frame, red_yuv_range[0], red_yuv_range[1])\n",
    "\n",
    "            hsv_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "            red_hsv_range = ((  0, 64, 64),\n",
    "                             (255,255,255))\n",
    "            red_hsv_mask = cv2.inRange(hsv_frame, red_hsv_range[0], red_hsv_range[1])\n",
    "            \n",
    "            red_mask = cv2.bitwise_and(red_yuv_mask, red_hsv_mask)\n",
    "\n",
    "            # -----------------------------------------------------------\n",
    "            # Dilate mask to make them survive from resizing\n",
    "            # -----------------------------------------------------------\n",
    "            red_mask = cv2.dilate(red_mask,\n",
    "                                  cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (9,9)),\n",
    "                                  iterations=3)\n",
    "\n",
    "            # -----------------------------------------------------------\n",
    "            # Resize and export frames\n",
    "            # -----------------------------------------------------------\n",
    "            out_frame = cv2.resize(red_mask, (28, 28))\n",
    "            out_filename = OUT_DATA_PATH + filename.replace('.MOV', '_%d.png' % frame_index)\n",
    "\n",
    "            cv2.imwrite(out_filename, out_frame)\n",
    "            frame_index += 1\n",
    "\n",
    "            print(\"\\rCreating (%d/%d)... %s         \" % (file_index+1, len(ls), out_filename), end=\"\")\n",
    "        video.release()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# 결론\n",
    "\n",
    "## 해석\n",
    "## 고찰\n",
    "## 계획\n",
    "\n",
    "---\n",
    "\n",
    "# 부록\n",
    "---\n",
    "## 참고\n",
    "* Teachable Machine: 심태섭 연구원이 소개해준 온라인 머신러닝 서비스로, 본 활동에 영감을 받음\n",
    "* "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit ('ml': conda)",
   "language": "python",
   "name": "python37764bitmlconda56395923f6734d2e9d871b41acf277cc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}