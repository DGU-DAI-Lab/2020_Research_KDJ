{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 정상/비정상 여부를 판단하기 위한 CNN 모델\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 모델 생성\n",
    "\n",
    "### 1-1. CNN 모델 설계하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from src import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(\n",
    "        filters=32,\n",
    "        kernel_size=(5,5),\n",
    "        padding='same',\n",
    "        activation='relu',\n",
    "        input_shape=(28,28,1)),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    Conv2D(\n",
    "        filters=32,\n",
    "        kernel_size=(3,3),\n",
    "        padding='same',\n",
    "        activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "    Dropout(0.25),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    Dense(16, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2. 데이터셋 불러오기 & 모델 학습\n",
    "\n",
    "* 연구 노트: '[2_데이터셋 생성하기.ipynb](./2_%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%85%8B%20%EC%83%9D%EC%84%B1%ED%95%98%EA%B8%B0.ipynb./2_%EB%8D%B0%EC%9D%B4%ED%84%B0%EC%85%8B%20%EC%83%9D%EC%84%B1%ED%95%98%EA%B8%B0.ipynb)'에서 생성한 데이터를 사용."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 360 samples\nEpoch 1/24\n360/360 [==============================] - 1s 2ms/sample - loss: 0.1120 - accuracy: 0.9583\nEpoch 2/24\n360/360 [==============================] - 1s 1ms/sample - loss: 0.1016 - accuracy: 0.9667\nEpoch 3/24\n360/360 [==============================] - 0s 1ms/sample - loss: 0.1047 - accuracy: 0.9694\nEpoch 4/24\n360/360 [==============================] - 0s 1ms/sample - loss: 0.1027 - accuracy: 0.9556\nEpoch 5/24\n360/360 [==============================] - 0s 1ms/sample - loss: 0.1177 - accuracy: 0.9611\nEpoch 6/24\n360/360 [==============================] - 0s 1ms/sample - loss: 0.1215 - accuracy: 0.9528\nEpoch 7/24\n360/360 [==============================] - 0s 1ms/sample - loss: 0.0994 - accuracy: 0.9583\nEpoch 8/24\n360/360 [==============================] - 0s 1ms/sample - loss: 0.0686 - accuracy: 0.9833\nEpoch 9/24\n360/360 [==============================] - 0s 1ms/sample - loss: 0.1199 - accuracy: 0.9417\nEpoch 10/24\n360/360 [==============================] - 0s 1ms/sample - loss: 0.0725 - accuracy: 0.9778\nEpoch 11/24\n360/360 [==============================] - 1s 2ms/sample - loss: 0.0654 - accuracy: 0.9861\nEpoch 12/24\n360/360 [==============================] - 0s 1ms/sample - loss: 0.0595 - accuracy: 0.9806\nEpoch 13/24\n360/360 [==============================] - 0s 1ms/sample - loss: 0.0651 - accuracy: 0.9722\nEpoch 14/24\n360/360 [==============================] - 0s 1ms/sample - loss: 0.0520 - accuracy: 0.9889\nEpoch 15/24\n360/360 [==============================] - 0s 974us/sample - loss: 0.0657 - accuracy: 0.9722\nEpoch 16/24\n360/360 [==============================] - 0s 1ms/sample - loss: 0.0723 - accuracy: 0.9750\nEpoch 17/24\n360/360 [==============================] - 0s 1ms/sample - loss: 0.0612 - accuracy: 0.9833\nEpoch 18/24\n360/360 [==============================] - 0s 1ms/sample - loss: 0.0499 - accuracy: 0.9861\nEpoch 19/24\n360/360 [==============================] - 0s 988us/sample - loss: 0.0685 - accuracy: 0.9722\nEpoch 20/24\n360/360 [==============================] - 0s 1ms/sample - loss: 0.0474 - accuracy: 0.9833\nEpoch 21/24\n360/360 [==============================] - 0s 1ms/sample - loss: 0.0526 - accuracy: 0.9833\nEpoch 22/24\n360/360 [==============================] - 0s 1ms/sample - loss: 0.0315 - accuracy: 0.9833\nEpoch 23/24\n360/360 [==============================] - 0s 1ms/sample - loss: 0.0294 - accuracy: 0.9944\nEpoch 24/24\n360/360 [==============================] - 0s 1ms/sample - loss: 0.0496 - accuracy: 0.9806\n90/90 [==============================] - 0s 267us/sample - loss: 0.1601 - accuracy: 0.9556\nINFO:tensorflow:Assets written to: model/softmax24/assets\n"
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = dataset.load()\n",
    "\n",
    "x_train = np.reshape(x_train / 255.0, tuple([x_train.shape[0]] + list(model.input_shape)[1:]))\n",
    "x_test  = np.reshape(x_test  / 255.0, tuple([x_test.shape[0]]  + list(model.input_shape)[1:]))\n",
    "\n",
    "model.fit(x_train, y_train, epochs=24)\n",
    "model.evaluate(x_test, y_test)\n",
    "\n",
    "model.save('model/softmax24')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~ps. 'model/model_32_epochs'은 훈련셋에서 0.9667, 테스트셋에서 0.9222로 오버피팅 발생.  \n",
    "아래서는 'model/model_16_epochs'를 사용하기로 함.~~\n",
    "\n",
    "| model | onTrain | onTest | desc |\n",
    "| --- | --- | --- | --- |\n",
    "| name(epochs) | accuracy(loss) | accuracy(loss) |  |\n",
    "| --- | --- | --- | --- |\n",
    "| softmax8 | 0.9556(0.1455) | 0.9333(0.2052) | Underfitting |\n",
    "| *softmax16 | 0.9583(0.1052) | 0.9556(0.1011) | *selected |\n",
    "| softmax24 | 0.9806(0.0496) | 0.9556(0.1601) | Overfitting |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. 구현한 모델을 사용하여 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import math\n",
    "import datetime\n",
    "\n",
    "from src import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "video file saved at [data/records/07_07-18-30.avi].\n"
    }
   ],
   "source": [
    "# 위 코드에 이어서...\n",
    "\n",
    "model = load_model('model/softmax16')\n",
    "\n",
    "data_list = list(dataset.tmp_load())\n",
    "\n",
    "mx, my = 0, 0\n",
    "win_name = 'predict model'\n",
    "data_info = {\n",
    "    'basename': '',\n",
    "    'frame' : {\n",
    "        'raw': None,\n",
    "        'mask': None,\n",
    "        'mask-colored': None\n",
    "    }\n",
    "}\n",
    "\n",
    "# --------------------------------\n",
    "\n",
    "def onMouse(event, x, y, flags, param):\n",
    "    \"\"\"마우스 포인터의 좌표 갱신\"\"\"\n",
    "    global mx, my\n",
    "    if event == cv2.EVENT_MOUSEMOVE:\n",
    "        mx, my = x, y\n",
    "\n",
    "def updateDataset(basename, raw, mask, mask_colored):\n",
    "    global data_info\n",
    "    data_info['basename'] = basename\n",
    "    data_info['frame']['raw'] = raw\n",
    "    data_info['frame']['mask'] = mask\n",
    "    data_info['frame']['mask-colored'] = mask_colored\n",
    "\n",
    "def makePoints(n, cx, cy, min_point, max_point):\n",
    "    \"\"\"중심의 좌표(cx,cy)와 한 변의 길이(n)을 이용하여\n",
    "    정사각형의 두 꼭짓점의 좌표를 계산.\"\"\"\n",
    "    x0, y0 = math.floor(cx - n/2), math.floor(cy - n/2)\n",
    "    x1, y1 = math.floor(cx + n/2), math.floor(cy + n/2)\n",
    "\n",
    "    # 범위 밖 좌표를 방지하기 위한 보정\n",
    "    dx, dy = 0, 0\n",
    "    padding = 4\n",
    "\n",
    "    minx, miny = min_point\n",
    "    maxx, maxy = max_point\n",
    "    \n",
    "    if (x0 < minx): dx = minx -x0 + padding\n",
    "    if (x1 > maxx): dx = maxx -x1 - padding\n",
    "\n",
    "    if (y0 < miny): dy = miny -y0 + padding\n",
    "    if (y1 > maxy): dy = maxy -y1 - padding\n",
    "\n",
    "    if (x1-x0) == (y1-y0) != n:\n",
    "        raise Exception('width height does not match?')\n",
    "\n",
    "    if dx == dy == 0:\n",
    "        return x0, y0, x1, y1\n",
    "    else:\n",
    "        return makePoints(n, cx+dx, cy+dy, min_point, max_point)\n",
    "\n",
    "# --------------------------------\n",
    "\n",
    "cv2.namedWindow(win_name)\n",
    "cv2.setMouseCallback(win_name, onMouse)\n",
    "\n",
    "updateDataset(*data_list[0])\n",
    "\n",
    "# --------------------------------\n",
    "# 비디오 녹화 준비\n",
    "\n",
    "height, width = data_info['frame']['raw'].shape[:2]\n",
    "height_padding = 32\n",
    "res = (width, height+height_padding)\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "now = datetime.datetime.now().strftime(\"%y-%m-%d_%H-%M-%S\")\n",
    "vw = cv2.VideoWriter(f\"data/records/{now}.avi\", fourcc, 20.0, res)\n",
    "\n",
    "# --------------------------------\n",
    "\n",
    "while True:\n",
    "    frame_info = data_info['frame']\n",
    "    # --------------------------------\n",
    "    # 출력할 프레임 전처리\n",
    "    h, w = frame_info['raw'].shape[:2]\n",
    "    x0,y0,x1,y1 = makePoints(28, mx, my, (0,0), (w,h))\n",
    "\n",
    "    frame_out = np.zeros((h+height_padding, w, 3), dtype=np.uint8)\n",
    "\n",
    "    frame_cut = cv2.cvtColor(frame_info['raw'][y0:y1, x0:x1], cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    frame_bg = frame_info['raw'].copy()\n",
    "    # frame_bg  = cv2.addWeighted(\n",
    "    #     frame_info['raw'],          0.75,\n",
    "    #     frame_info['mask-colored'], 0.75, 0)\n",
    "    cv2.rectangle(frame_bg, (x0,y0), (x1,y1), (0,0,255), 1)\n",
    "\n",
    "    frame_out[:h,:] = frame_bg\n",
    "    frame_out[-30:-2,2:30] = cv2.cvtColor(frame_cut, cv2.COLOR_GRAY2BGR)\n",
    "    # --------------------------------\n",
    "    # 모델을 이용한 예측\n",
    "    x_to_predict = (frame_cut / 255.0).reshape(*model.input_shape[1:])\n",
    "    y_predicted  = model.predict(np.expand_dims(x_to_predict, axis=0))[0]\n",
    "    # --------------------------------\n",
    "    # 출력할 프레임 후처리\n",
    "    predicted_probability = max(y_predicted)\n",
    "    predicted_label = np.where(y_predicted == predicted_probability)[0][0]\n",
    "    predicted_label_title = ['normal', 'broken'][predicted_label]\n",
    "\n",
    "    cv2.putText(frame_out, f\"{predicted_label_title}, Prob: {predicted_probability*100:.4f}%\",\n",
    "                (32, h+12), cv2.FONT_HERSHEY_PLAIN, 1.0, (0,0,255), lineType=cv2.LINE_AA)\n",
    "\n",
    "    cv2.putText(frame_out, str(y_predicted),\n",
    "                (32, h+24), cv2.FONT_HERSHEY_PLAIN, 0.8, (0,0,255), lineType=cv2.LINE_AA)\n",
    "    # --------------------------------\n",
    "    cv2.imshow(win_name, frame_out)\n",
    "    vw.write(frame_out)\n",
    "\n",
    "    key = chr(cv2.waitKey(10) & 0xFF)\n",
    "    if key == 'q':\n",
    "        break\n",
    "\n",
    "vw.release()\n",
    "cv2.imshow(win_name, frame_out)\n",
    "cv2.destroyWindow(win_name)\n",
    "\n",
    "print(f\"video file saved at [data/records/{now}.avi].\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37764bitmlconda56395923f6734d2e9d871b41acf277cc",
   "display_name": "Python 3.7.7 64-bit ('ml': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}